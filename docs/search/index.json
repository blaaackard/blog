[{"content":"æœ€åˆã« çš†ã•ã‚“ã€\u0026ldquo;K8sGPT\u0026rdquo; çŸ¥ã£ã¦ã„ã¾ã™ã‹ï¼Ÿä½¿ã£ãŸã“ã¨ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\nGitHub: https://github.com/k8sgpt-ai/k8sgpt/tree/main\nCloud Native Days FUKUOKA 2023ã«å‚åŠ ã—ãŸéš›ã«ã€K8sGPTã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è´è¬›ã—ã¦èˆˆå‘³ã‚’æŒã£ãŸã®ã§å®Ÿéš›ã«è§¦ã£ã¦ã¿ã¾ã—ãŸã€‚\nã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±: https://event.cloudnativedays.jp/cndf2023/talks/1885\nK8sGPT ã¯ Kubernetes ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®å•é¡Œã‚’æ¢ç´¢ã—ã€è©•ä¾¡ã€è§£èª¬ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚\nã‚·ã‚¹ãƒ†ãƒ ä¿¡é ¼æ€§ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°(SRE)ã®å°‚é–€çŸ¥è­˜ãŒçµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãªçŸ¥è­˜ã«è‡ªä¿¡ãŒãªã„äººã§ã‚‚å®¹æ˜“ã«ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ã€‚\nå®Ÿè¡Œè‡ªä½“ã¯æœ¬å½“ã«ã‚·ãƒ³ãƒ—ãƒ«ã§k8sgpt analyzeã¨ã„ã†ã‚³ãƒãƒ³ãƒ‰ã‚’å©ãã ã‘ã§å®Ÿè¡Œã§ãã¾ã™ã€‚\nå®Ÿéš›ã«ã‚¯ãƒ©ã‚¹ã‚¿ã«å•é¡Œã‚’ç™ºç”Ÿã•ã›ã¦æ¤œçŸ¥ã—ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ï¼\nã‚¯ãƒ©ã‚¹ã‚¿ã¨å•é¡Œãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã®ä½œæˆ Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã®ä½œæˆãŒå¿…è¦ãªæ–¹ã¯ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚\nã¾ãŸã€K8sGPTã«å•é¡Œã‚’æ¤œçŸ¥ã•ã›ã‚‹ãŸã‚ã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’æº–å‚™ã—ã¾ã™ã€‚\nä»Šå›ã¯ãƒãƒ¼ãƒ‰ã®å‰²å½“å¯èƒ½CPUã‚’è¶…éã•ã›ã‚‹ã‚ˆã†ãªã‚‚ã®ã‚’ChatGPTã«ä½œã£ã¦ã‚‚ã‚‰ã„ã¾ã—ãŸğŸ¤–\nsample.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion: apps/v1 kind: Deployment metadata: name: faulty-deployment namespace: k8sgpt spec: replicas: 3 selector: matchLabels: app: faulty-app template: metadata: labels: app: faulty-app spec: containers: - name: faulty-container image: nginx:latest resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: faulty-service namespace: k8sgpt spec: selector: app: faulty-app ports: - protocol: TCP port: 80 targetPort: 8080 CLIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« æ—©é€ŸREADMEã«è¨˜è¼‰ã®é€šã‚Šbrewã§å…¥ã‚Œã¦ã„ãã¾ã™ã€‚\n1 2 â¯ brew tap k8sgpt-ai/k8sgpt â¯ brew install k8sgpt APIã‚­ãƒ¼ã®ç™ºè¡Œ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ãŸã‚‰APIã‚­ãƒ¼ã‚’ç™ºè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\nk8sgpt generateã‚’æ‰“ã¤ã¨OpenAIã®ãƒšãƒ¼ã‚¸ã«é·ç§»ã—ã¾ã—ãŸã€‚\nğŸš¨OpenAIã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆãŒå¿…è¦ã§ã™ã€‚\nğŸš¨å¾Œç¶šã®åˆ†æã§ã¯CreditãŒå¿…è¦ã«ãªã‚Šã¾ã™(Min5$ã‹ã‚‰è³¼å…¥ã§ãã¾ã™)ã€‚\n1 2 3 4 â¯ k8sgpt generate # Opening: https://beta.openai.com/account/api-keys to generate a key for openai # Please copy the generated key and run `k8sgpt auth` to add it to your config file + Create new secret keyã‚’æŠ¼ã™ã¨ã‚­ãƒ¼ã®åå‰ã‚’å…¥åŠ›ã™ã‚‹ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ãŒå‡ºã¦ãã‚‹ã®ã§ã€é©å½“ãªåå‰ã‚’å…¥åŠ›ã—ã¦Create secret keyã‚’æŠ¼ã—ã¾ã™ã€‚\nã‚­ãƒ¼ãŒç™ºè¡Œã•ã‚Œã‚‹ã®ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚\nProviderã®è¨­å®š 1 2 3 â¯ k8sgpt auth add -b openai -m gpt-3.5-turbo -p {COPYã—ãŸAPIã‚­ãƒ¼} # openai added to the AI backend provider list ã“ã“ã§ã„ã† -bã¯ã€Œbackend AI providerã€-mã¯ã€Œbackend AI modelã€ã‚’æ„å‘³ã—ã¦ã„ã¾ã™ã€‚\nREADMEé€šã‚Šã®k8sgpt auth addã ã‘ã ã¨k8sgpt analyzeã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦åˆ†æè‡ªä½“ã¯ã§ãã‚‹ã®ã§ã™ãŒã€explainã‚„Filterã‚’ä½¿ãŠã†ã¨ã™ã‚‹ã¨æ€’ã‚‰ã‚ŒãŸã®ã§æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¾ã—ãŸã€‚\n1 2 3 4 5 6 7 8 â¯ k8sgpt auth add # Warning: backend input is empty, will use the default value: openai # Warning: model input is empty, will use the default value: gpt-3.5-turbo Enter openai Key: # openai added to the AI backend provider list â¯ k8sgpt analyze --explain --namespace=k8sgpt # 0% | (0/12, 0 it/hr) [0s:0s] # Error: failed while calling AI provider openai: error, status code: 400, message: you must provide a model parameter ã¡ãªã¿ã«ä»Šå›ã¯OpenAIã‚’æŒ‡å®šã—ã¾ã—ãŸãŒã€ã‚ªãƒ©ã‚¯ãƒ«ãŒæ”¯æ´ã™ã‚‹Cohereç­‰ã‚‚ä½¿ãˆãã†ã§ã—ãŸã€‚\n1 2 3 4 5 6 7 8 9 10 â¯ k8sgpt auth list # Default: # \u0026gt; openai # Active: # \u0026gt; openai # Unused: # \u0026gt; localai # \u0026gt; azureopenai # \u0026gt; noopai # \u0026gt; cohere ã„ã–åˆ†æï¼ åˆ†æè‡ªä½“ã¯k8sgpt analyzeã‚’ãŸãŸãã ã‘ã§ã™ã€‚\n\u0026ndash;namespace={NAME} ã‚„ \u0026ndash;filter=Pod ãªã©çµã‚Šè¾¼ã¿ã‚‚ã§ãã¾ã™ã€‚\nServiceã¯ãƒ©ãƒ™ãƒ«ã‚’è²¼ã‚ã†ã¨è¨€ã‚ã‚Œã¦ãŠã‚Šã€Deploymentã¯CPUãƒªã‚½ãƒ¼ã‚¹ãŒä¸è¶³ã—ã¦ã„ã‚‹ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™ã€‚\nã¡ã‚ƒã‚“ã¨CPUã«é–¢ã™ã‚‹åˆ†æçµæœãŒå‡ºåŠ›ã•ã‚Œã¦å®‰å¿ƒã—ã¾ã—ãŸâœŒ\n1 2 3 4 5 6 7 â¯ k8sgpt analyze --namespace=k8sgpt # AI Provider: openai # 0 k8sgpt/faulty-service(faulty-service) # - Error: Service has no endpoints, expected label app=faulty-app # 1 k8sgpt/faulty-deployment-6879cc8f7f-cntv2(Deployment/faulty-deployment) # - Error: 0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.. \u0026ndash;explain ã‚’ã¤ã‘ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€å•é¡Œã®ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 â¯ k8sgpt analyze --explain --namespace=k8sgpt # 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (4/4, 12 it/min) # AI Provider: openai # 0 k8sgpt/faulty-service(faulty-service) # - Error: Service has no endpoints, expected label app=faulty-app # Error: The service has no endpoints and it is expecting the label app=faulty-app. # Solution: # 1. Check the labels of the faulty-app deployment. # 2. Make sure the labels match the selector in the service definition. # 3. Update the labels if necessary. # 4. Restart the deployment and service to apply the changes. # 1 k8sgpt/faulty-deployment-6879cc8f7f-cntv2(Deployment/faulty-deployment) # - Error: 0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.. # Error: 0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.. # Solution: # 1. Check the resource requirements of the pod. # 2. Increase the CPU limits or request values in the pod\u0026#39;s configuration. # 3. Ensure that the cluster has enough available CPU resources. # 4. If necessary, add more nodes to the cluster. # 5. Retry deploying the pod. -l Japanese ã‚’ã¤ã‘ã‚‹ã“ã¨ã§æ—¥æœ¬èªåŒ–ã‚‚ã§ãã¡ã‚ƒã„ã¾ã™ã€‚\nã€ŒçŠ ç‰²è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€\u0026hellip;æ©Ÿæ¢°çš„ãªç¿»è¨³ã«ãªã£ã¦ã¾ã™ãŒæ„å‘³ã¯ç†è§£ã§ãã¾ã™ã­ğŸ™†â€â™‚ï¸\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 â¯ k8sgpt analyze --explain -l Japanese --namespace=k8sgpt # 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (4/4, 9 it/min) # AI Provider: openai # 0 k8sgpt/faulty-service(faulty-service) # - Error: Service has no endpoints, expected label app=faulty-app # Error: ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ©ãƒ™ãƒ«app=faulty-appãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚ # Solution: # 1. kubectl get svcã‚’ä½¿ç”¨ã—ã¦ç¾åœ¨ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç¢ºèªã—ã¾ã™ã€‚ # 2. kubectl label svc \u0026lt;service-name\u0026gt; app=faulty-appã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ¼ãƒ“ã‚¹ã«ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã—ã¾ã™ã€‚ # 3. kubectl get endpointsã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒæ­£ã—ãè¿½åŠ ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚ # 1 k8sgpt/faulty-deployment-6879cc8f7f-cntv2(Deployment/faulty-deployment) # - Error: 0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.. # Error: 0/2 ãƒãƒ¼ãƒ‰ãŒåˆ©ç”¨å¯èƒ½ã§ã™: 2 CPU ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ—ãƒªã‚¨ãƒ³ãƒ—ã‚·ãƒ§ãƒ³: 0/2 ãƒãƒ¼ãƒ‰ãŒåˆ©ç”¨å¯èƒ½ã§ã™: 2 å…¥åŠ›ãƒãƒƒãƒ‰ã®ãƒ—ãƒªã‚¨ãƒ³ãƒ—ã‚·ãƒ§ãƒ³ã®çŠ ç‰²è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ # Solution: # 1. ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒãƒ¼ãƒ‰ã® CPU ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¾ã™ã€‚ # 2. ãƒãƒ¼ãƒ‰ãŒååˆ†ãª CPU ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚ # 3. ã‚‚ã—ãã¯ã€ãƒãƒƒãƒ‰ã® CPU ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ãŸã¯åˆ¶é™ã‚’èª¿æ•´ã—ã¾ã™ã€‚ # 4. ã‚‚ã—ãã¯ã€ä»–ã®ãƒãƒ¼ãƒ‰ã«ãƒãƒƒãƒ‰ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã—ã¾ã™ã€‚ å•é¡Œã‚’ä¿®æ­£ã—ã¦å†ãƒ‡ãƒ—ãƒ­ã‚¤ K8sGPTã®åˆ†æçµæœã‚’å‚è€ƒã«ä»¥ä¸‹ã®ä¿®æ­£ã‚’è¡Œã„ã¾ã™ã€‚\nfaulty-serviceã«ãƒ©ãƒ™ãƒ«ã‚’è²¼ã‚‹\n(Solutionã§ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’å‡ºåŠ›ã—ã¦ãã‚Œã¦ã‚‹ã®ã§ãã‚Œé€šã‚Šã«å®Ÿè¡Œã—ã¾ã™ã€‚) 1 2 â¯ kubectl label svc faulty-service app=faulty-app -n k8sgpt # service/faulty-service labeled faulty-deploymentã®CPUãƒªã‚½ãƒ¼ã‚¹ãƒªãƒŸãƒƒãƒˆå€¤ã‚’Nodeã®å‰²å½“å¯èƒ½ãªå¤§ãã•ã¾ã§ä¸‹ã’ã‚‹ 1 2 3 4 5 6 7 8 spec: containers: - name: faulty-container image: nginx:latest resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; # \u0026gt;\u0026gt;\u0026#34;50m\u0026#34;ã«å¤‰æ›´ å†åº¦åˆ†æã‚’ã™ã‚‹ã¨ã€ã€ã€å•é¡ŒãŒãªããªã‚Šã¾ã—ãŸğŸ‰\n1 2 3 4 â¯ k8sgpt analyze --explain -n k8sgpt #AI Provider: openai #No problems detected ã¾ã¨ã‚ ä»Šå›ã¯Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã«å•é¡Œã‚’ç™ºç”Ÿã•ã›ã€å®Ÿéš›ã«K8sGPTã‚’ä½¿ç”¨ã—ã¦å•é¡Œã‚’åˆ†æã—ã€ä¿®æ­£ã¾ã§ã‚’ã‚„ã£ã¦ã¿ã¾ã—ãŸã€‚\nç°¡å˜ã«ã‚¯ãƒ©ã‚¹ã‚¿ã®åˆ†æãŒã§ãã‚‹ã“ã¨ãŒãŠåˆ†ã‹ã‚Šã„ãŸã ã‘ãŸã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ï¼\nK8sGPTã¯ã¾ã ãã“ã¾ã§ä¸€èˆ¬çš„ã«ãªã£ã¦ã„ã¾ã›ã‚“ãŒã€ç©æ¥µçš„ãªé–‹ç™ºãŒé€²ã‚“ã§ã„ã¾ã™ã€‚\nä»Šå¾Œã®KubernetesÃ—AIã®è©±é¡Œã«ã¯æ³¨ç›®ã§ã™ã­ğŸ‘€\næœ€å¾Œã¾ã§èª­ã‚“ã§ãã ã•ã‚Šã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚\n","date":"2023-10-07T00:17:43+09:00","image":"https://blaaackard.github.io/blog/p/k8s-gpt/cover_hu55a857987ee61949054669d80844a31c_2738055_120x120_fill_box_smart1_3.png","permalink":"https://blaaackard.github.io/blog/p/k8s-gpt/","title":"K8sGPTã‚’ä½¿ã£ã¦ã‚¯ãƒ©ã‚¹ã‚¿ã®AIåˆ†æã‚’ã—ã¦ã¿ã‚ˆã†"}]