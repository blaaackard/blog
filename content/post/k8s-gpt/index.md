---
title: "K8sGPTã‚’ä½¿ã£ã¦ã‚¯ãƒ©ã‚¹ã‚¿ã®AIåˆ†æã‚’ã—ã¦ã¿ã‚ˆã†"
description: CNDF2023ã§çŸ¥ã£ãŸK8sGPTã‚’ä»Šæ›´ãªãŒã‚‰ä½¿ã£ã¦ã¿ã‚‹ğŸ˜¼
slug: k8s-gpt
date: 2023-10-07T00:17:43+09:00
image: cover.png
math: 
license: 
hidden: false
comments: true
draft: false
categories:
    - Kubernetes
tags:
    - k8sgpt
    - Kubernetes
    - AI
---

# æœ€åˆã«

**çš†ã•ã‚“ã€"K8sGPT" çŸ¥ã£ã¦ã„ã¾ã™ã‹ï¼Ÿä½¿ã£ãŸã“ã¨ã‚ã‚Šã¾ã™ã‹ï¼Ÿ**  
GitHub: https://github.com/k8sgpt-ai/k8sgpt/tree/main

Cloud Native Days FUKUOKA 2023ã«å‚åŠ ã—ãŸéš›ã«ã€K8sGPTã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è´è¬›ã—ã¦èˆˆå‘³ã‚’æŒã£ãŸã®ã§å®Ÿéš›ã«è§¦ã£ã¦ã¿ã¾ã—ãŸã€‚  
ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±: https://event.cloudnativedays.jp/cndf2023/talks/1885

K8sGPT ã¯ Kubernetes ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®å•é¡Œã‚’æ¢ç´¢ã—ã€è©•ä¾¡ã€è§£èª¬ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚  
ã‚·ã‚¹ãƒ†ãƒ ä¿¡é ¼æ€§ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°(SRE)ã®å°‚é–€çŸ¥è­˜ãŒçµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãªçŸ¥è­˜ã«è‡ªä¿¡ãŒãªã„äººã§ã‚‚å®¹æ˜“ã«ä½¿ã†ã“ã¨ãŒã§ãã¾ã™ã€‚  
å®Ÿè¡Œè‡ªä½“ã¯æœ¬å½“ã«ã‚·ãƒ³ãƒ—ãƒ«ã§`k8sgpt analyze`ã¨ã„ã†ã‚³ãƒãƒ³ãƒ‰ã‚’å©ãã ã‘ã§å®Ÿè¡Œã§ãã¾ã™ã€‚

**å®Ÿéš›ã«ã‚¯ãƒ©ã‚¹ã‚¿ã«å•é¡Œã‚’ç™ºç”Ÿã•ã›ã¦æ¤œçŸ¥ã—ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ï¼**

## ã‚¯ãƒ©ã‚¹ã‚¿ã¨å•é¡Œãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã®ä½œæˆ

Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã®ä½œæˆãŒå¿…è¦ãªæ–¹ã¯ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚  
ã¾ãŸã€K8sGPTã«å•é¡Œã‚’æ¤œçŸ¥ã•ã›ã‚‹ãŸã‚ã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’æº–å‚™ã—ã¾ã™ã€‚  
ä»Šå›ã¯ãƒãƒ¼ãƒ‰ã®å‰²å½“å¯èƒ½CPUã‚’è¶…éã•ã›ã‚‹ã‚ˆã†ãªã‚‚ã®ã‚’ChatGPTã«ä½œã£ã¦ã‚‚ã‚‰ã„ã¾ã—ãŸğŸ¤–


<details>
<summary>sample.yaml</summary>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: faulty-deployment
  namespace: k8sgpt
spec:
  replicas: 3
  selector:
    matchLabels:
      app: faulty-app
  template:
    metadata:
      labels:
        app: faulty-app
    spec:
      containers:
      - name: faulty-container
        image: nginx:latest
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080

---

apiVersion: v1
kind: Service
metadata:
  name: faulty-service
  namespace: k8sgpt
spec:
  selector:
    app: faulty-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080

```

</details>


## CLIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

æ—©é€ŸREADMEã«è¨˜è¼‰ã®é€šã‚Šbrewã§å…¥ã‚Œã¦ã„ãã¾ã™ã€‚

```bash
â¯ brew tap k8sgpt-ai/k8sgpt
â¯ brew install k8sgpt
```

## APIã‚­ãƒ¼ã®ç™ºè¡Œ

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ãŸã‚‰APIã‚­ãƒ¼ã‚’ç™ºè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚  
`k8sgpt generate`ã‚’æ‰“ã¤ã¨OpenAIã®ãƒšãƒ¼ã‚¸ã«é·ç§»ã—ã¾ã—ãŸã€‚

ğŸš¨**OpenAIã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆãŒå¿…è¦ã§ã™ã€‚**  
ğŸš¨**å¾Œç¶šã®åˆ†æã§ã¯CreditãŒå¿…è¦ã«ãªã‚Šã¾ã™(Min5$ã‹ã‚‰è³¼å…¥ã§ãã¾ã™)ã€‚**

```bash
â¯ k8sgpt generate

# Opening: https://beta.openai.com/account/api-keys to generate a key for openai
# Please copy the generated key and run `k8sgpt auth` to add it to your config file
```

![api-keyç™ºè¡Œç”»é¢](API-key-1.png)

`+ Create new secret key`ã‚’æŠ¼ã™ã¨ã‚­ãƒ¼ã®åå‰ã‚’å…¥åŠ›ã™ã‚‹ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ãŒå‡ºã¦ãã‚‹ã®ã§ã€é©å½“ãªåå‰ã‚’å…¥åŠ›ã—ã¦`Create secret key`ã‚’æŠ¼ã—ã¾ã™ã€‚  
ã‚­ãƒ¼ãŒç™ºè¡Œã•ã‚Œã‚‹ã®ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚

## Providerã®è¨­å®š

```bash
â¯ k8sgpt auth add -b openai -m gpt-3.5-turbo -p {COPYã—ãŸAPIã‚­ãƒ¼}

# openai added to the AI backend provider list
```

ã“ã“ã§ã„ã† -bã¯ã€Œbackend AI providerã€-mã¯ã€Œbackend AI modelã€ã‚’æ„å‘³ã—ã¦ã„ã¾ã™ã€‚  
READMEé€šã‚Šã®`k8sgpt auth add`ã ã‘ã ã¨`k8sgpt analyze`ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦åˆ†æè‡ªä½“ã¯ã§ãã‚‹ã®ã§ã™ãŒã€explainã‚„Filterã‚’ä½¿ãŠã†ã¨ã™ã‚‹ã¨æ€’ã‚‰ã‚ŒãŸã®ã§æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¾ã—ãŸã€‚

```bash
â¯ k8sgpt auth add
# Warning: backend input is empty, will use the default value: openai
# Warning: model input is empty, will use the default value: gpt-3.5-turbo
Enter openai Key: # openai added to the AI backend provider list

â¯ k8sgpt analyze --explain --namespace=k8sgpt
#   0% |                        (0/12, 0 it/hr) [0s:0s]
# Error: failed while calling AI provider openai: error, status code: 400, message: you must provide a model parameter
```

ã¡ãªã¿ã«ä»Šå›ã¯OpenAIã‚’æŒ‡å®šã—ã¾ã—ãŸãŒã€ã‚ªãƒ©ã‚¯ãƒ«ãŒæ”¯æ´ã™ã‚‹Cohereç­‰ã‚‚ä½¿ãˆãã†ã§ã—ãŸã€‚

```bash
â¯ k8sgpt auth list
# Default:
# > openai
# Active:
# > openai
# Unused:
# > localai
# > azureopenai
# > noopai
# > cohere
```

## ã„ã–åˆ†æï¼

åˆ†æè‡ªä½“ã¯`k8sgpt analyze`ã‚’ãŸãŸãã ã‘ã§ã™ã€‚  
--namespace={NAME} ã‚„ --filter=Pod  ãªã©çµã‚Šè¾¼ã¿ã‚‚ã§ãã¾ã™ã€‚  
Serviceã¯ãƒ©ãƒ™ãƒ«ã‚’è²¼ã‚ã†ã¨è¨€ã‚ã‚Œã¦ãŠã‚Šã€Deploymentã¯CPUãƒªã‚½ãƒ¼ã‚¹ãŒä¸è¶³ã—ã¦ã„ã‚‹ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™ã€‚  
ã¡ã‚ƒã‚“ã¨CPUã«é–¢ã™ã‚‹åˆ†æçµæœãŒå‡ºåŠ›ã•ã‚Œã¦å®‰å¿ƒã—ã¾ã—ãŸâœŒ

```bash
â¯ k8sgpt analyze --namespace=k8sgpt
# AI Provider: openai

# 0 k8sgpt/faulty-service(faulty-service)
# - Error: Service has no endpoints, expected label app=faulty-app
# 1 k8sgpt/faulty-deployment-6879cc8f7f-cntv2(Deployment/faulty-deployment)
# - Error: 0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..
```

--explain ã‚’ã¤ã‘ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€å•é¡Œã®ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```bash
â¯ k8sgpt analyze --explain --namespace=k8sgpt
# 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (4/4, 12 it/min)
# AI Provider: openai

# 0 k8sgpt/faulty-service(faulty-service)
# - Error: Service has no endpoints, expected label app=faulty-app
# Error: The service has no endpoints and it is expecting the label app=faulty-app.
# Solution:
# 1. Check the labels of the faulty-app deployment.
# 2. Make sure the labels match the selector in the service definition.
# 3. Update the labels if necessary.
# 4. Restart the deployment and service to apply the changes.
# 1 k8sgpt/faulty-deployment-6879cc8f7f-cntv2(Deployment/faulty-deployment)
# - Error: 0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..
# Error: 0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..
# Solution:
# 1. Check the resource requirements of the pod.
# 2. Increase the CPU limits or request values in the pod's configuration.
# 3. Ensure that the cluster has enough available CPU resources.
# 4. If necessary, add more nodes to the cluster.
# 5. Retry deploying the pod.
```
-l Japanese ã‚’ã¤ã‘ã‚‹ã“ã¨ã§æ—¥æœ¬èªåŒ–ã‚‚ã§ãã¡ã‚ƒã„ã¾ã™ã€‚  
ã€ŒçŠ ç‰²è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€...æ©Ÿæ¢°çš„ãªç¿»è¨³ã«ãªã£ã¦ã¾ã™ãŒæ„å‘³ã¯ç†è§£ã§ãã¾ã™ã­ğŸ™†â€â™‚ï¸

```bash
â¯ k8sgpt analyze --explain -l Japanese --namespace=k8sgpt
# 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (4/4, 9 it/min)
# AI Provider: openai

# 0 k8sgpt/faulty-service(faulty-service)
# - Error: Service has no endpoints, expected label app=faulty-app
# Error: ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ©ãƒ™ãƒ«app=faulty-appãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚
# Solution:
# 1. kubectl get svcã‚’ä½¿ç”¨ã—ã¦ç¾åœ¨ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç¢ºèªã—ã¾ã™ã€‚
# 2. kubectl label svc <service-name> app=faulty-appã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ¼ãƒ“ã‚¹ã«ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ã—ã¾ã™ã€‚
# 3. kubectl get endpointsã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒæ­£ã—ãè¿½åŠ ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
# 1 k8sgpt/faulty-deployment-6879cc8f7f-cntv2(Deployment/faulty-deployment)
# - Error: 0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..
# Error: 0/2 ãƒãƒ¼ãƒ‰ãŒåˆ©ç”¨å¯èƒ½ã§ã™: 2 CPU ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ—ãƒªã‚¨ãƒ³ãƒ—ã‚·ãƒ§ãƒ³: 0/2 ãƒãƒ¼ãƒ‰ãŒåˆ©ç”¨å¯èƒ½ã§ã™: 2 å…¥åŠ›ãƒãƒƒãƒ‰ã®ãƒ—ãƒªã‚¨ãƒ³ãƒ—ã‚·ãƒ§ãƒ³ã®çŠ ç‰²è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚
# Solution:
# 1. ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒãƒ¼ãƒ‰ã® CPU ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¾ã™ã€‚
# 2. ãƒãƒ¼ãƒ‰ãŒååˆ†ãª CPU ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
# 3. ã‚‚ã—ãã¯ã€ãƒãƒƒãƒ‰ã® CPU ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ãŸã¯åˆ¶é™ã‚’èª¿æ•´ã—ã¾ã™ã€‚
# 4. ã‚‚ã—ãã¯ã€ä»–ã®ãƒãƒ¼ãƒ‰ã«ãƒãƒƒãƒ‰ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã—ã¾ã™ã€‚
```

## å•é¡Œã‚’ä¿®æ­£ã—ã¦å†ãƒ‡ãƒ—ãƒ­ã‚¤

K8sGPTã®åˆ†æçµæœã‚’å‚è€ƒã«ä»¥ä¸‹ã®ä¿®æ­£ã‚’è¡Œã„ã¾ã™ã€‚  
- faulty-serviceã«ãƒ©ãƒ™ãƒ«ã‚’è²¼ã‚‹  
(Solutionã§ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’å‡ºåŠ›ã—ã¦ãã‚Œã¦ã‚‹ã®ã§ãã‚Œé€šã‚Šã«å®Ÿè¡Œã—ã¾ã™ã€‚)

```bash
â¯ kubectl label svc faulty-service app=faulty-app -n k8sgpt
# service/faulty-service labeled
```
- faulty-deploymentã®CPUãƒªã‚½ãƒ¼ã‚¹ãƒªãƒŸãƒƒãƒˆå€¤ã‚’Nodeã®å‰²å½“å¯èƒ½ãªå¤§ãã•ã¾ã§ä¸‹ã’ã‚‹
```yaml
spec:
  containers:
  - name: faulty-container
    image: nginx:latest
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m" # >>"50m"ã«å¤‰æ›´
```

å†åº¦åˆ†æã‚’ã™ã‚‹ã¨ã€ã€ã€å•é¡ŒãŒãªããªã‚Šã¾ã—ãŸğŸ‰

```bash
â¯ k8sgpt analyze --explain -n k8sgpt
#AI Provider: openai

#No problems detected
```

# ã¾ã¨ã‚

ä»Šå›ã¯Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã«å•é¡Œã‚’ç™ºç”Ÿã•ã›ã€å®Ÿéš›ã«K8sGPTã‚’ä½¿ç”¨ã—ã¦å•é¡Œã‚’åˆ†æã—ã€ä¿®æ­£ã¾ã§ã‚’ã‚„ã£ã¦ã¿ã¾ã—ãŸã€‚  
ç°¡å˜ã«ã‚¯ãƒ©ã‚¹ã‚¿ã®åˆ†æãŒã§ãã‚‹ã“ã¨ãŒãŠåˆ†ã‹ã‚Šã„ãŸã ã‘ãŸã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ï¼  
ä»Šå›ã¯å°è¦æ¨¡ã‚¯ãƒ©ã‚¹ã‚¿ ã‹ã¤ Namespace ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ã‹ã‘ã¦æ„å›³çš„ã«åˆ†æçµæœã‚’å‡ºã—ã¦ã‚‚ã‚‰ã„ã¾ã—ãŸãŒã€å¤§è¦æ¨¡ã‚¯ãƒ©ã‚¹ã‚¿ã®å…¨ä½“æ¤œç´¢ã¨ã‹ã—ã¦ã¿ã‚‹ã¨ä¸é©åˆ‡ãªè¨­å®šå€¤ã®ã¾ã¾æ”¾ç½®ã•ã‚Œã¦ã„ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã®ç™ºè¦‹ç­‰ãŒã§ããã†ã§ã™ã­ï¼  
K8sGPTã¯ã¾ã ãã“ã¾ã§ä¸€èˆ¬çš„ã«ãªã£ã¦ã„ã¾ã›ã‚“ãŒã€ç©æ¥µçš„ãªé–‹ç™ºãŒé€²ã‚“ã§ã„ã¾ã™ã€‚ä»Šå¾Œã®KubernetesÃ—AIã®è©±é¡Œã«ã¯æ³¨ç›®ã§ã™ã­ğŸ‘€

æœ€å¾Œã¾ã§èª­ã‚“ã§ãã ã•ã‚Šã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚

